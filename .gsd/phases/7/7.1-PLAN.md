---
phase: 7
plan: 1
wave: 1
gap_closure: true
---

# Plan 7.1: Infrastructure & Test Robustness

## Objective
Harden the codebase against regression by centralizing mocks and institutionalizing performance monitoring.

## Tasks

<task type="auto">
  <name>Centralize WordNet Mocks</name>
  <files>
    tests/conftest.py
    tests/test_integration_pipeline.py
    tests/test_fast_preview.py
  </files>
  <action>
    1. Create `tests/conftest.py` if it doesn't exist.
    2. Define a `mock_wn` fixture and a `mock_synset_factory`.
    3. Refactor integration tests to use these common fixtures instead of redeclaring complex `MagicMock` setups.
  </action>
  <verify>uv run pytest tests/test_integration_pipeline.py</verify>
  <done>Tests pass using shared fixtures.</done>
</task>

<task type="auto">
  <name>Relax Brittle Assertions</name>
  <files>
    tests/test_shaper.py
    tests/test_fast_preview.py
  </files>
  <action>
    1. Identify assertions in `test_shaper.py` that depend on exact mock object identity or specific call counts.
    2. Replace with broader assertions (e.g., checking final structure keys/values) to tolerate internal logic optimizations.
  </action>
  <verify>uv run pytest tests/test_shaper.py</verify>
  <done>Tests are robust to minor shaper behavior changes.</done>
</task>

<task type="auto">
  <name>Automated Performance CI</name>
  <files>
    pyproject.toml
    .github/workflows/perf.yml
  </files>
  <action>
    1. Add a `benchmark` script to `pyproject.toml`.
    2. (Optional) Create a simple GitHub action that runs `tests/test_benchmark_preview.py` and fails if latency exceeds 500ms.
  </action>
  <verify>npm run benchmark (via uv/python adapter)</verify>
  <done>Performance benchmark is easily executable and verified.</done>
</task>

## Success Criteria
- [ ] Integration tests pass with 50% less boilerplate.
- [ ] Benchmarks confirm <0.5s for "Fast Preview" on standard subtrees.
