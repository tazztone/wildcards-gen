---
phase: 4
plan: fix-perf-gaps
wave: 1
gap_closure: true
---

# Fix: WordNet Caching & Performance Verification

## Problem
- Audit of Phase 3 shows we implemented UMAP caching but didn't cache expensive WordNet graph traversals (`get_all_descendants`), which `ImageNet` generation relies on.
- We lack empirical proof that "Deep Tuning" is sub-second.

## Root Cause
- Phase 3 focused on the heaviest hitter (UMAP) but missed secondary bottlenecks (graph traversal).
- Verification was "manual" in plan but requires automation for confidence.

## Tasks

<task type="auto">
  <name>Cache WordNet Traversals</name>
  <files>wildcards_gen/core/wordnet.py</files>
  <action>
    Add `@functools.lru_cache(maxsize=100)` to `get_all_descendants`.
    Note: Traversals return lists, so we might need to convert to tuple for cache ability or ensure args are hashable (synset is hashable, set of strings is not -> frozenset).
    Update signature if needed to accept `frozenset` for `valid_wnids`.
  </action>
  <verify>grep "lru_cache" wildcards_gen/core/wordnet.py</verify>
  <done>Function is cached</done>
</task>

<task type="auto">
  <name>Benchmark Preview Latency</name>
  <files>tests/test_benchmark_preview.py</files>
  <action>
    Create a benchmark test that:
    1. Runs a full generation (priming cache).
    2. Runs it again with different UMAP params.
    3. Asserts duration < 1.0s (excluding initial load).
  </action>
  <verify>uv run pytest tests/test_benchmark_preview.py</verify>
  <done>Performance proven < 1s</done>
</task>

## Success Criteria
- [ ] `get_all_descendants` is cached.
- [ ] Re-runs with UMAP tuning take < 1s.
